{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c941fe0-e853-44c5-ab03-281cf744c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub URL: https://github.com/punampaul04/Deep_Learning_Week05_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b18b720-b0d9-44b3-84ee-5b3ff3ff8032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution enabled.\n",
      "Loading images from: /Users/vedangmehta/Downloads/dog_images/working/Images\n",
      "1000/20580 images loaded.\n",
      "2000/20580 images loaded.\n",
      "3000/20580 images loaded.\n",
      "4000/20580 images loaded.\n",
      "5000/20580 images loaded.\n",
      "6000/20580 images loaded.\n",
      "7000/20580 images loaded.\n",
      "8000/20580 images loaded.\n",
      "9000/20580 images loaded.\n",
      "10000/20580 images loaded.\n",
      "11000/20580 images loaded.\n",
      "12000/20580 images loaded.\n",
      "13000/20580 images loaded.\n",
      "14000/20580 images loaded.\n",
      "15000/20580 images loaded.\n",
      "16000/20580 images loaded.\n",
      "17000/20580 images loaded.\n",
      "18000/20580 images loaded.\n",
      "19000/20580 images loaded.\n",
      "20000/20580 images loaded.\n",
      "Number of images loaded: 20580\n",
      "Dataset shape: (20580, 64, 64, 3)\n",
      "Building generator...\n",
      "Generator built successfully.\n",
      "Building discriminator...\n",
      "Discriminator built successfully.\n",
      "Optimizers and loss functions defined.\n",
      "Starting GAN training...\n",
      "Epoch 1/5 starting...\n",
      "Step 0 | D Loss: 0.7018 | G Loss: 0.7160\n",
      "Step 50 | D Loss: 0.0038 | G Loss: 7.2601\n",
      "Step 100 | D Loss: 0.2222 | G Loss: 17.3667\n",
      "Step 150 | D Loss: 0.0036 | G Loss: 5.6079\n",
      "Epoch 1/5 completed | Avg D Loss: 0.0916 | Avg G Loss: 6.7337\n",
      "Epoch 2/5 starting...\n",
      "Step 0 | D Loss: 0.0038 | G Loss: 5.5816\n",
      "Step 50 | D Loss: 0.1939 | G Loss: 4.6337\n",
      "Step 100 | D Loss: 0.0122 | G Loss: 4.7497\n",
      "Step 150 | D Loss: 0.0031 | G Loss: 5.5670\n",
      "Epoch 2/5 completed | Avg D Loss: 0.0325 | Avg G Loss: 4.8492\n",
      "Epoch 3/5 starting...\n",
      "Step 0 | D Loss: 0.0050 | G Loss: 5.4574\n",
      "Step 50 | D Loss: 0.0021 | G Loss: 6.6833\n",
      "Step 100 | D Loss: 0.0013 | G Loss: 7.0403\n",
      "Step 150 | D Loss: 0.0010 | G Loss: 7.6982\n",
      "Epoch 3/5 completed | Avg D Loss: 0.0025 | Avg G Loss: 6.8344\n",
      "Epoch 4/5 starting...\n",
      "Step 0 | D Loss: 0.0012 | G Loss: 7.7741\n",
      "Step 50 | D Loss: 0.0007 | G Loss: 8.3299\n",
      "Step 100 | D Loss: 0.0005 | G Loss: 8.6058\n",
      "Step 150 | D Loss: 0.0005 | G Loss: 8.9242\n",
      "Epoch 4/5 completed | Avg D Loss: 0.0008 | Avg G Loss: 8.5024\n",
      "Epoch 5/5 starting...\n",
      "Step 0 | D Loss: 0.0004 | G Loss: 8.9687\n",
      "Step 50 | D Loss: 0.0006 | G Loss: 9.3391\n",
      "Step 100 | D Loss: 0.0003 | G Loss: 9.5333\n",
      "Step 150 | D Loss: 0.0002 | G Loss: 9.7047\n",
      "Epoch 5/5 completed | Avg D Loss: 0.0004 | Avg G Loss: 9.4926\n",
      "GAN training completed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "## Generative Dog Images Using a GAN with a Custom Training Loop\n",
    "\n",
    "**Objective:**\n",
    "Train a Generative Adversarial Network (GAN) to generate realistic dog images from a dataset of dog images.\n",
    "\n",
    "**Approach:**\n",
    "- Utilize a custom training loop with `tf.GradientTape()` to manually control forward and backward passes, avoiding internal Keras trainer issues.\n",
    "- The generator creates images from random noise.\n",
    "- The discriminator classifies images as real or fake.\n",
    "- The GAN (generator + frozen discriminator) trains the generator to produce images that the discriminator deems real.\n",
    "\n",
    "**Dataset:**\n",
    "- **Path Structure:** `/Users/vedangmehta/Downloads/dog_images/working/Images/<breed_name>/<image_file>.jpg`\n",
    "- **Preprocessing:** Images are resized to 64x64 pixels and normalized to the range [0, 1].\n",
    "\n",
    "**Evaluation:**\n",
    "- **Metric:** MiFID (Memorization-informed Fr√©chet Inception Distance). Lower values indicate better performance.\n",
    "\n",
    "**Note:**\n",
    "Ensure that TensorFlow 2.x is installed in your environment. This code is designed to run in environments where eager execution is enabled.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, \n",
    "    Conv2D, Conv2DTranspose, Dropout, Input\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Enable Eager Execution\n",
    "# ----------------------------\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "print(\"Eager execution enabled.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Data Loading and Preprocessing\n",
    "# ----------------------------\n",
    "\n",
    "DATA_DIR = '/Users/vedangmehta/Downloads/dog_images'\n",
    "WORKING_DIR = os.path.join(DATA_DIR, 'working/Images')\n",
    "\n",
    "def load_dog_images(image_dir, target_size=(64, 64)):\n",
    "    print(f\"Loading images from: {image_dir}\")\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.jpg'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    images = []\n",
    "    for i, path in enumerate(image_paths):\n",
    "        try:\n",
    "            img = load_img(path, target_size=target_size)\n",
    "            img_array = img_to_array(img) / 255.0\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"{i + 1}/{len(image_paths)} images loaded.\")\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "dog_images = load_dog_images(WORKING_DIR)\n",
    "print(f\"Number of images loaded: {len(dog_images)}\")\n",
    "print(f\"Dataset shape: {dog_images.shape}\")\n",
    "\n",
    "if len(dog_images) == 0:\n",
    "    raise ValueError(\"No images found. Please check the dataset extraction and paths.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Building the GAN Components\n",
    "# ----------------------------\n",
    "\n",
    "def build_generator():\n",
    "    print(\"Building generator...\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8 * 8 * 256, input_dim=100))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "    print(\"Generator built successfully.\")\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_shape=(64, 64, 3)):\n",
    "    print(\"Building discriminator...\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\", input_shape=input_shape))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    print(\"Discriminator built successfully.\")\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Defining Optimizers and Loss Functions\n",
    "# ----------------------------\n",
    "\n",
    "generator_optimizer = Adam(0.0002, 0.5)\n",
    "discriminator_optimizer = Adam(0.0002, 0.5)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "print(\"Optimizers and loss functions defined.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Custom Training Loop\n",
    "# ----------------------------\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    real_labels = tf.ones((batch_size, 1))\n",
    "    fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "    with tf.GradientTape() as tape_d:\n",
    "        real_preds = discriminator(real_images, training=True)\n",
    "        d_loss_real = bce(real_labels, real_preds)\n",
    "\n",
    "        fake_images = generator(noise, training=True)\n",
    "        fake_preds = discriminator(fake_images, training=True)\n",
    "        d_loss_fake = bce(fake_labels, fake_preds)\n",
    "\n",
    "        d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "\n",
    "    grads_d = tape_d.gradient(d_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape_g:\n",
    "        gen_images = generator(noise, training=True)\n",
    "        gen_preds = discriminator(gen_images, training=False)\n",
    "        g_loss = bce(real_labels, gen_preds)\n",
    "\n",
    "    grads_g = tape_g.gradient(g_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "\n",
    "    return d_loss, g_loss\n",
    "\n",
    "def train_gan(generator, discriminator, dataset, epochs=20, batch_size=128):\n",
    "    print(\"Starting GAN training...\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices(dataset).shuffle(buffer_size=10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs} starting...\")\n",
    "        d_losses, g_losses = [], []\n",
    "        for step, real_batch in enumerate(ds):\n",
    "            d_loss, g_loss = train_step(real_batch)\n",
    "            d_losses.append(d_loss)\n",
    "            g_losses.append(g_loss)\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step} | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "        avg_d_loss = tf.reduce_mean(d_losses).numpy()\n",
    "        avg_g_loss = tf.reduce_mean(g_losses).numpy()\n",
    "        print(f\"Epoch {epoch}/{epochs} completed | Avg D Loss: {avg_d_loss:.4f} | Avg G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "train_gan(generator, discriminator, dog_images, epochs=5, batch_size=128)\n",
    "print(\"GAN training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
